---
title: 构建文本处理工作流
icon: workflow
order: 11
---

# 构建文本处理工作流

本文将通过实战项目，详细介绍如何构建高效的文本处理工作流，整合grep、sed、awk等工具，结合管道和重定向技术，实现复杂的文本分析和处理任务，提高文本处理效率和自动化程度。

## 项目目标

- 掌握文本处理工具链的组合使用
- 构建可重用的文本处理模块
- 实现复杂数据提取和转换
- 自动化日常文本处理任务

## 文本处理工作流设计原则

高效的文本处理工作流应遵循以下设计原则：

1. **模块化**：将复杂任务分解为独立的处理步骤
2. **可重用性**：设计通用组件，便于在不同场景复用
3. **管道化**：充分利用Unix管道机制连接各处理环节
4. **错误处理**：加入适当的错误检测和处理机制
5. **性能优化**：针对大文件处理进行优化设计

## 文本处理工具链基础

在构建文本处理工作流之前，我们需要了解各个工具的核心功能和适用场景，这样才能在工作流中合理选择和组合工具。

### 核心工具功能概览

| 工具 | 主要功能 | 最适合的场景 |
|------|---------|------------|
| grep | 文本搜索和过滤 | 查找匹配特定模式的行 |
| sed | 流编辑和文本替换 | 文本转换和替换操作 |
| awk | 结构化数据处理 | 字段提取和数据计算 |
| sort | 文本排序 | 对数据进行排序和去重 |
| cut | 列提取 | 提取固定格式文本的特定列 |
| tr | 字符转换 | 字符级别的替换和删除 |
| uniq | 去除重复行 | 数据去重和计数 |
| wc | 计数统计 | 统计行数、单词数和字符数 |

### 管道和重定向

管道（`|`）和重定向（`>`、`>>`、`<`）是构建文本处理工作流的关键机制：

```bash
# 管道：将前一个命令的输出作为后一个命令的输入
command1 | command2 | command3

# 输出重定向：将命令输出保存到文件
command > output.txt      # 覆盖写入
command >> output.txt     # 追加写入

# 输入重定向：从文件读取输入
command < input.txt

# 错误重定向：重定向标准错误
command 2> error.log
command > output.txt 2>&1  # 同时重定向标准输出和标准错误
```

## 实战项目一：日志分析工作流

### 项目需求

分析Web服务器访问日志，提取关键信息，生成访问统计报告。

### 输入数据

假设我们有以下格式的Apache访问日志：

```
192.168.1.10 - - [15/Jul/2023:10:30:15 +0800] "GET /index.html HTTP/1.1" 200 2048 "http://example.com" "Mozilla/5.0"
192.168.1.15 - - [15/Jul/2023:10:31:20 +0800] "POST /login HTTP/1.1" 302 0 "http://example.com/login" "Mozilla/5.0"
192.168.1.10 - - [15/Jul/2023:10:32:45 +0800] "GET /dashboard HTTP/1.1" 200 4096 "http://example.com" "Mozilla/5.0"
10.0.0.5 - - [15/Jul/2023:10:33:10 +0800] "GET /images/logo.png HTTP/1.1" 200 1024 "http://example.com" "Mozilla/5.0"
192.168.1.20 - - [15/Jul/2023:10:35:22 +0800] "GET /api/data HTTP/1.1" 404 0 "http://example.com/app" "Mozilla/5.0"
```

### 工作流设计

我们将构建一个多阶段的日志分析工作流：

1. 提取特定时间段的日志
2. 过滤出错误请求（4xx和5xx状态码）
3. 统计每个IP的访问次数
4. 分析最常访问的URL路径
5. 生成综合报告

### 实现步骤

#### 1. 提取特定时间段的日志

```bash
# 提取10:30到10:35之间的日志
grep "\[15/Jul/2023:10:3[0-5]:" access.log > filtered_by_time.log
```

#### 2. 过滤出错误请求

```bash
# 提取状态码为4xx或5xx的请求
grep -E "HTTP/1\.[01]\" [45][0-9][0-9]" filtered_by_time.log > error_requests.log
```

#### 3. 统计每个IP的访问次数

```bash
# 提取IP地址并计数
awk '{print $1}' filtered_by_time.log | sort | uniq -c | sort -nr > ip_counts.txt
```

#### 4. 分析最常访问的URL路径

```bash
# 提取URL路径并计数
awk '{print $7}' filtered_by_time.log | sort | uniq -c | sort -nr > url_counts.txt
```

#### 5. 生成综合报告

```bash
# 创建报告头部
echo "Web服务器访问日志分析报告" > report.txt
echo "生成时间: $(date)" >> report.txt
echo "分析时间段: 15/Jul/2023 10:30-10:35" >> report.txt
echo "" >> report.txt

# 添加总请求数
echo "总请求数: $(wc -l < filtered_by_time.log)" >> report.txt
echo "错误请求数: $(wc -l < error_requests.log)" >> report.txt
echo "" >> report.txt

# 添加IP统计
echo "访问IP统计 (前5名):" >> report.txt
head -5 ip_counts.txt >> report.txt
echo "" >> report.txt

# 添加URL统计
echo "访问URL统计 (前5名):" >> report.txt
head -5 url_counts.txt >> report.txt
```

### 完整工作流脚本

将上述步骤整合为一个完整的Shell脚本：

```bash
#!/bin/bash

# 日志分析工作流脚本
# 用法: ./analyze_logs.sh access.log

if [ $# -ne 1 ]; then
    echo "用法: $0 access_log_file"
    exit 1
fi

LOG_FILE=$1
TEMP_DIR="./temp_$(date +%s)"
REPORT_FILE="report_$(date +%Y%m%d).txt"

# 创建临时目录
mkdir -p $TEMP_DIR

# 步骤1: 提取特定时间段的日志
echo "正在提取时间段内的日志..."
grep "\[15/Jul/2023:10:3[0-5]:" $LOG_FILE > $TEMP_DIR/filtered_by_time.log

# 步骤2: 过滤出错误请求
echo "正在过滤错误请求..."
grep -E "HTTP/1\.[01]\" [45][0-9][0-9]" $TEMP_DIR/filtered_by_time.log > $TEMP_DIR/error_requests.log

# 步骤3: 统计每个IP的访问次数
echo "正在统计IP访问次数..."
awk '{print $1}' $TEMP_DIR/filtered_by_time.log | sort | uniq -c | sort -nr > $TEMP_DIR/ip_counts.txt

# 步骤4: 分析最常访问的URL路径
echo "正在分析URL访问情况..."
awk '{print $7}' $TEMP_DIR/filtered_by_time.log | sort | uniq -c | sort -nr > $TEMP_DIR/url_counts.txt

# 步骤5: 生成综合报告
echo "正在生成报告..."
{
    echo "Web服务器访问日志分析报告"
    echo "=========================="
    echo "生成时间: $(date)"
    echo "分析时间段: 15/Jul/2023 10:30-10:35"
    echo "分析文件: $LOG_FILE"
    echo ""
    
    echo "1. 总体统计"
    echo "------------"
    echo "总请求数: $(wc -l < $TEMP_DIR/filtered_by_time.log)"
    echo "错误请求数: $(wc -l < $TEMP_DIR/error_requests.log)"
    echo "错误率: $(awk "BEGIN {printf \"%.2f%%\", ($(wc -l < $TEMP_DIR/error_requests.log) / $(wc -l < $TEMP_DIR/filtered_by_time.log)) * 100}")"
    echo ""
    
    echo "2. IP访问统计 (前5名)"
    echo "--------------------"
    head -5 $TEMP_DIR/ip_counts.txt
    echo ""
    
    echo "3. URL访问统计 (前5名)"
    echo "--------------------"
    head -5 $TEMP_DIR/url_counts.txt
    echo ""
    
    echo "4. 错误请求详情 (前10条)"
    echo "-----------------------"
    head -10 $TEMP_DIR/error_requests.log
} > $REPORT_FILE

echo "报告已生成: $REPORT_FILE"

# 清理临时文件
rm -rf $TEMP_DIR

echo "分析完成!"
```

## 实战项目二：数据转换工作流

### 项目需求

将CSV格式的销售数据转换为结构化的HTML报表，并进行数据清洗和汇总计算。

### 输入数据

假设我们有以下格式的CSV销售数据：

```
日期,产品ID,产品名称,价格,数量,客户ID
2023-07-01,P001,笔记本电脑,5999.00,2,C1001
2023-07-01,P002,鼠标,99.00,5,C1002
2023-07-02,P003,键盘,299.00,3,C1003
2023-07-02,P001,笔记本电脑,5999.00,1,C1004
2023-07-03,P004,显示器,1499.00,2,C1002
```

### 工作流设计

我们将构建一个数据转换工作流：

1. 数据清洗（处理缺失值、格式错误等）
2. 按日期分组计算销售额
3. 按产品分组计算销售量和销售额
4. 生成HTML格式的销售报表

### 实现步骤

#### 1. 数据清洗

```bash
# 检查并处理缺失值和格式问题
awk -F, '
    NR == 1 {print; next}  # 打印标题行
    {
        # 检查字段数量
        if (NF != 6) {
            print "错误行: " $0 > "/dev/stderr"
            next
        }
        
        # 检查日期格式
        if ($1 !~ /^[0-9]{4}-[0-9]{2}-[0-9]{2}$/) {
            print "日期格式错误: " $0 > "/dev/stderr"
            next
        }
        
        # 检查价格和数量是否为数字
        if ($4 !~ /^[0-9]+(\.[0-9]+)?$/ || $5 !~ /^[0-9]+$/) {
            print "价格或数量格式错误: " $0 > "/dev/stderr"
            next
        }
        
        print
    }
' sales.csv > cleaned_sales.csv
```

#### 2. 按日期分组计算销售额

```bash
# 跳过标题行，按日期分组计算销售额
awk -F, '
    NR > 1 {
        # 计算每行的销售额
        amount = $4 * $5
        
        # 按日期累加销售额
        daily_sales[$1] += amount
        
        # 记录总销售额
        total_sales += amount
    }
    
    END {
        print "日期,销售额"
        for (date in daily_sales) {
            printf "%s,%.2f\n", date, daily_sales[date]
        }
        printf "总计,%.2f\n", total_sales
    }
' cleaned_sales.csv > daily_sales.csv
```

#### 3. 按产品分组计算销售量和销售额

```bash
# 按产品分组计算销售量和销售额
awk -F, '
    NR > 1 {
        # 累加每个产品的销售量和销售额
        product_qty[$3] += $5
        product_amount[$3] += $4 * $5
    }
    
    END {
        print "产品名称,销售量,销售额"
        for (product in product_qty) {
            printf "%s,%d,%.2f\n", product, product_qty[product], product_amount[product]
        }
    }
' cleaned_sales.csv > product_sales.csv
```

#### 4. 生成HTML格式的销售报表

```bash
# 生成HTML报表
{
    # HTML头部
    echo '<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>销售数据报表</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        h1 { color: #333; }
        table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        tr:nth-child(even) { background-color: #f9f9f9; }
    </style>
</head>
<body>
    <h1>销售数据报表</h1>
    <h2>生成时间: '$(date)'</h2>'
    
    # 每日销售额表格
    echo '
    <h2>每日销售额</h2>
    <table>
        <tr>
            <th>日期</th>
            <th>销售额</th>
        </tr>'
    
    # 添加每日销售数据行
    awk -F, '
        NR > 1 {
            print "        <tr>"
            print "            <td>" $1 "</td>"
            print "            <td>￥" $2 "</td>"
            print "        </tr>"
        }
    ' daily_sales.csv
    
    echo '    </table>'
    
    # 产品销售表格
    echo '
    <h2>产品销售统计</h2>
    <table>
        <tr>
            <th>产品名称</th>
            <th>销售量</th>
            <th>销售额</th>
        </tr>'
    
    # 添加产品销售数据行
    awk -F, '
        NR > 1 {
            print "        <tr>"
            print "            <td>" $1 "</td>"
            print "            <td>" $2 "</td>"
            print "            <td>￥" $3 "</td>"
            print "        </tr>"
        }
    ' product_sales.csv
    
    echo '    </table>'
    
    # HTML尾部
    echo '</body>
</html>'
} > sales_report.html
```

### 完整工作流脚本

将上述步骤整合为一个完整的Shell脚本：

```bash
#!/bin/bash

# 销售数据转换工作流脚本
# 用法: ./generate_sales_report.sh sales.csv

if [ $# -ne 1 ]; then
    echo "用法: $0 sales_data.csv"
    exit 1
fi

CSV_FILE=$1
TEMP_DIR="./temp_$(date +%s)"
REPORT_FILE="sales_report_$(date +%Y%m%d).html"

# 创建临时目录
mkdir -p $TEMP_DIR

echo "开始处理销售数据..."

# 步骤1: 数据清洗
echo "正在清洗数据..."
awk -F, '
    NR == 1 {print; next}  # 打印标题行
    {
        # 检查字段数量
        if (NF != 6) {
            print "错误行: " $0 > "/dev/stderr"
            next
        }
        
        # 检查日期格式
        if ($1 !~ /^[0-9]{4}-[0-9]{2}-[0-9]{2}$/) {
            print "日期格式错误: " $0 > "/dev/stderr"
            next
        }
        
        # 检查价格和数量是否为数字
        if ($4 !~ /^[0-9]+(\.[0-9]+)?$/ || $5 !~ /^[0-9]+$/) {
            print "价格或数量格式错误: " $0 > "/dev/stderr"
            next
        }
        
        print
    }
' $CSV_FILE > $TEMP_DIR/cleaned_sales.csv

# 步骤2: 按日期分组计算销售额
echo "正在计算每日销售额..."
awk -F, '
    NR > 1 {
        # 计算每行的销售额
        amount = $4 * $5
        
        # 按日期累加销售额
        daily_sales[$1] += amount
        
        # 记录总销售额
        total_sales += amount
    }
    
    END {
        print "日期,销售额"
        for (date in daily_sales) {
            printf "%s,%.2f\n", date, daily_sales[date]
        }
        printf "总计,%.2f\n", total_sales
    }
' $TEMP_DIR/cleaned_sales.csv > $TEMP_DIR/daily_sales.csv

# 步骤3: 按产品分组计算销售量和销售额
echo "正在计算产品销售统计..."
awk -F, '
    NR > 1 {
        # 累加每个产品的销售量和销售额
        product_qty[$3] += $5
        product_amount[$3] += $4 * $5
    }
    
    END {
        print "产品名称,销售量,销售额"
        for (product in product_qty) {
            printf "%s,%d,%.2f\n", product, product_qty[product], product_amount[product]
        }
    }
' $TEMP_DIR/cleaned_sales.csv > $TEMP_DIR/product_sales.csv

# 步骤4: 生成HTML格式的销售报表
echo "正在生成HTML报表..."
{
    # HTML头部
    echo '<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>销售数据报表</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        h1 { color: #333; }
        table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        tr:nth-child(even) { background-color: #f9f9f9; }
        .total-row { font-weight: bold; background-color: #e6f7ff; }
    </style>
</head>
<body>
    <h1>销售数据报表</h1>
    <h2>生成时间: '$(date)'</h2>'
    
    # 每日销售额表格
    echo '
    <h2>每日销售额</h2>
    <table>
        <tr>
            <th>日期</th>
            <th>销售额</th>
        </tr>'
    
    # 添加每日销售数据行
    awk -F, '
        NR > 1 {
            if ($1 == "总计") {
                print "        <tr class=\"total-row\">"
            } else {
                print "        <tr>"
            }
            print "            <td>" $1 "</td>"
            print "            <td>￥" $2 "</td>"
            print "        </tr>"
        }
    ' $TEMP_DIR/daily_sales.csv
    
    echo '    </table>'
    
    # 产品销售表格
    echo '
    <h2>产品销售统计</h2>
    <table>
        <tr>
            <th>产品名称</th>
            <th>销售量</th>
            <th>销售额</th>
        </tr>'
    
    # 添加产品销售数据行
    awk -F, '
        NR > 1 {
            print "        <tr>"
            print "            <td>" $1 "</td>"
            print "            <td>" $2 "</td>"
            print "            <td>￥" $3 "</td>"
            print "        </tr>"
        }
    ' $TEMP_DIR/product_sales.csv
    
    echo '    </table>'
    
    # HTML尾部
    echo '</body>
</html>'
} > $REPORT_FILE

echo "报表已生成: $REPORT_FILE"

# 清理临时文件
rm -rf $TEMP_DIR

echo "处理完成!"
```

## 实战项目三：系统监控工作流

### 项目需求

构建一个系统监控工作流，定期收集系统资源使用情况，分析性能瓶颈，并生成监控报告。

### 工作流设计

我们将构建一个系统监控工作流：

1. 收集CPU、内存、磁盘和网络使用情况
2. 分析系统日志中的错误和警告
3. 检测异常进程和资源使用
4. 生成系统健康报告

### 实现步骤

#### 1. 收集系统资源使用情况

```bash
#!/bin/bash

# 创建输出目录
OUTPUT_DIR="./system_stats_$(date +%Y%m%d)"
mkdir -p $OUTPUT_DIR

# 收集CPU信息
echo "正在收集CPU信息..."
{
    echo "CPU使用情况 - $(date)"
    echo "--------------------"
    top -bn1 | head -20
    echo ""
    
    echo "CPU负载 - $(date)"
    echo "--------------------"
    uptime
    echo ""
    
    echo "CPU详细信息 - $(date)"
    echo "--------------------"
    cat /proc/cpuinfo | grep -E "processor|model name|cpu MHz|cache size"
} > $OUTPUT_DIR/cpu_info.txt

# 收集内存信息
echo "正在收集内存信息..."
{
    echo "内存使用情况 - $(date)"
    echo "--------------------"
    free -h
    echo ""
    
    echo "内存详细信息 - $(date)"
    echo "--------------------"
    cat /proc/meminfo | head -20
} > $OUTPUT_DIR/memory_info.txt

# 收集磁盘信息
echo "正在收集磁盘信息..."
{
    echo "磁盘使用情况 - $(date)"
    echo "--------------------"
    df -h
    echo ""
    
    echo "磁盘I/O统计 - $(date)"
    echo "--------------------"
    iostat -x 1 5
} > $OUTPUT_DIR/disk_info.txt

# 收集网络信息
echo "正在收集网络信息..."
{
    echo "网络连接情况 - $(date)"
    echo "--------------------"
    netstat -tuln
    echo ""
    
    echo "网络接口统计 - $(date)"
    echo "--------------------"
    ifconfig
    echo ""
    
    echo "网络流量统计 - $(date)"
    echo "--------------------"
    sar -n DEV 1 5
} > $OUTPUT_DIR/network_info.txt

echo "系统资源信息已收集到 $OUTPUT_DIR 目录"
```

#### 2. 分析系统日志中的错误和警告

```bash
#!/bin/bash

# 分析系统日志
LOG_DIR="/var/log"
OUTPUT_DIR="./system_stats_$(date +%Y%m%d)"
mkdir -p $OUTPUT_DIR

echo "正在分析系统日志..."
{
    echo "系统日志错误分析 - $(date)"
    echo "------------------------"
    
    echo "1. 系统错误信息 (最近100条)"
    echo "------------------------"
    grep -i "error\|fail\|critical" $LOG_DIR/syslog | tail -100
    echo ""
    
    echo "2. 认证失败尝试"
    echo "------------------------"
    grep -i "failed\|failure" $LOG_DIR/auth.log | tail -50
    echo ""
    
    echo "3. 内核错误信息"
    echo "------------------------"
    dmesg | grep -i "error\|fail\|warn" | tail -50
    echo ""
    
    echo "4. 应用程序错误 (Apache/Nginx)"
    echo "------------------------"
    if [ -f "$LOG_DIR/apache2/error.log" ]; then
        grep -i "error" $LOG_DIR/apache2/error.log | tail -50
    elif [ -f "$LOG_DIR/nginx/error.log" ]; then
        grep -i "error" $LOG_DIR/nginx/error.log | tail -50
    else
        echo "未找到Web服务器错误日志"
    fi
} > $OUTPUT_DIR/log_analysis.txt

echo "日志分析完成，结果保存在 $OUTPUT_DIR/log_analysis.txt"
```

#### 3. 检测异常进程和资源使用

```bash
#!/bin/bash

OUTPUT_DIR="./system_stats_$(date +%Y%m%d)"
mkdir -p $OUTPUT_DIR

echo "正在检测异常进程和资源使用..."
{
    echo "异常进程和资源使用分析 - $(date)"
    echo "------------------------------"
    
    echo "1. CPU使用率最高的进程 (前10个)"
    echo "------------------------------"
    ps aux | sort -nrk 3,3 | head -10
    echo ""
    
    echo "2. 内存使用率最高的进程 (前10个)"
    echo "------------------------------"
    ps aux | sort -nrk 4,4 | head -10
    echo ""
    
    echo "3. 运行时间最长的进程 (前10个)"
    echo "------------------------------"
    ps -eo pid,user,comm,etime,args | sort -nrk 4,4 | head -10
    echo ""
    
    echo "4. 僵尸进程检测"
    echo "------------------------------"
    ps aux | awk '$8=="Z"' | if [ -z "$(cat)" ]; then echo "没有僵尸进程"; else cat; fi
    echo ""
    
    echo "5. 打开文件数最多的进程 (前5个)"
    echo "------------------------------"
    for pid in $(ps -eo pid --no-headers | head -5); do
        echo "进程 $pid ($(ps -p $pid -o comm=)) 打开的文件数: $(ls -l /proc/$pid/fd 2>/dev/null | wc -l)"
    done
} > $OUTPUT_DIR/process_analysis.txt

echo "进程分析完成，结果保存在 $OUTPUT_DIR/process_analysis.txt"
```

#### 4. 生成系统健康报告

```bash
#!/bin/bash

OUTPUT_DIR="./system_stats_$(date +%Y%m%d)"
REPORT_FILE="$OUTPUT_DIR/system_health_report.html"

echo "正在生成系统健康报告..."

# 提取CPU使用率
CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print $2 + $4}')
CPU_STATUS="正常"
if (( $(echo "$CPU_USAGE > 80" | bc -l) )); then
    CPU_STATUS="警告"
elif (( $(echo "$CPU_USAGE > 90" | bc -l) )); then
    CPU_STATUS="危险"
fi

# 提取内存使用率
MEM_TOTAL=$(free | grep Mem | awk '{print $2}')
MEM_USED=$(free | grep Mem | awk '{print $3}')
MEM_USAGE=$(echo "scale=2; $MEM_USED*100/$MEM_TOTAL" | bc)
MEM_STATUS="正常"
if (( $(echo "$MEM_USAGE > 80" | bc -l) )); then
    MEM_STATUS="警告"
elif (( $(echo "$MEM_USAGE > 90" | bc -l) )); then
    MEM_STATUS="危险"
fi

# 提取磁盘使用率
DISK_USAGE=$(df -h | grep -vE "tmpfs|cdrom" | awk '{ print $5 让我继续完成"构建文本处理工作流"的内容：

---
title: 构建文本处理工作流
icon: workflow
order: 11
---

# 构建文本处理工作流

本文将通过实战项目，详细介绍如何构建高效的文本处理工作流，整合grep、sed、awk等工具，结合管道和重定向技术，实现复杂的文本分析和处理任务，提高文本处理效率和自动化程度。

## 项目目标

- 掌握文本处理工具链的组合使用
- 构建可重用的文本处理模块
- 实现复杂数据提取和转换
- 自动化日常文本处理任务

## 文本处理工作流设计原则

高效的文本处理工作流应遵循以下设计原则：

1. **模块化**：将复杂任务分解为独立的处理步骤
2. **可重用性**：设计通用组件，便于在不同场景复用
3. **管道化**：充分利用Unix管道机制连接各处理环节
4. **错误处理**：加入适当的错误检测和处理机制
5. **性能优化**：针对大文件处理进行优化设计

## 文本处理工具链基础

在构建文本处理工作流之前，我们需要了解各个工具的核心功能和适用场景，这样才能在工作流中合理选择和组合工具。

### 核心工具功能概览

| 工具 | 主要功能 | 最适合的场景 |
|------|---------|------------|
| grep | 文本搜索和过滤 | 查找匹配特定模式的行 |
| sed | 流编辑和文本替换 | 文本转换和替换操作 |
| awk | 结构化数据处理 | 字段提取和数据计算 |
| sort | 文本排序 | 对数据进行排序和去重 |
| cut | 列提取 | 提取固定格式文本的特定列 |
| tr | 字符转换 | 字符级别的替换和删除 |
| uniq | 去除重复行 | 数据去重和计数 |
| wc | 计数统计 | 统计行数、单词数和字符数 |

### 管道和重定向

管道（`|`）和重定向（`>`、`>>`、`<`）是构建文本处理工作流的关键机制：

```bash
# 管道：将前一个命令的输出作为后一个命令的输入
command1 | command2 | command3

# 输出重定向：将命令输出保存到文件
command > output.txt      # 覆盖写入
command >> output.txt     # 追加写入

# 输入重定向：从文件读取输入
command < input.txt

# 错误重定向：重定向标准错误
command 2> error.log
command > output.txt 2>&1  # 同时重定向标准输出和标准错误
```

## 实战项目一：日志分析工作流

### 项目需求

分析Web服务器访问日志，提取关键信息，生成访问统计报告。

### 输入数据

假设我们有以下格式的Apache访问日志：

```
192.168.1.10 - - [15/Jul/2023:10:30:15 +0800] "GET /index.html HTTP/1.1" 200 2048 "http://example.com" "Mozilla/5.0"
192.168.1.15 - - [15/Jul/2023:10:31:20 +0800] "POST /login HTTP/1.1" 302 0 "http://example.com/login" "Mozilla/5.0"
192.168.1.10 - - [15/Jul/2023:10:32:45 +0800] "GET /dashboard HTTP/1.1" 200 4096 "http://example.com" "Mozilla/5.0"
10.0.0.5 - - [15/Jul/2023:10:33:10 +0800] "GET /images/logo.png HTTP/1.1" 200 1024 "http://example.com" "Mozilla/5.0"
192.168.1.20 - - [15/Jul/2023:10:35:22 +0800] "GET /api/data HTTP/1.1" 404 0 "http://example.com/app" "Mozilla/5.0"
```

### 工作流设计

我们将构建一个多阶段的日志分析工作流：

1. 提取特定时间段的日志
2. 过滤出错误请求（4xx和5xx状态码）
3. 统计每个IP的访问次数
4. 分析最常访问的URL路径
5. 生成综合报告

### 实现步骤

#### 1. 提取特定时间段的日志

```bash
# 提取10:30到10:35之间的日志
grep "\[15/Jul/2023:10:3[0-5]:" access.log > filtered_by_time.log
```

#### 2. 过滤出错误请求

```bash
# 提取状态码为4xx或5xx的请求
grep -E "HTTP/1\.[01]\" [45][0-9][0-9]" filtered_by_time.log > error_requests.log
```

#### 3. 统计每个IP的访问次数

```bash
# 提取IP地址并计数
awk '{print $1}' filtered_by_time.log | sort | uniq -c | sort -nr > ip_counts.txt
```

#### 4. 分析最常访问的URL路径

```bash
# 提取URL路径并计数
awk '{print $7}' filtered_by_time.log | sort | uniq -c | sort -nr > url_counts.txt
```

#### 5. 生成综合报告

```bash
# 创建报告头部
echo "Web服务器访问日志分析报告" > report.txt
echo "生成时间: $(date)" >> report.txt
echo "分析时间段: 15/Jul/2023 10:30-10:35" >> report.txt
echo "" >> report.txt

# 添加总请求数
echo "总请求数: $(wc -l < filtered_by_time.log)" >> report.txt
echo "错误请求数: $(wc -l < error_requests.log)" >> report.txt
echo "" >> report.txt

# 添加IP统计
echo "访问IP统计 (前5名):" >> report.txt
head -5 ip_counts.txt >> report.txt
echo "" >> report.txt

# 添加URL统计
echo "访问URL统计 (前5名):" >> report.txt
head -5 url_counts.txt >> report.txt
```

### 完整工作流脚本

将上述步骤整合为一个完整的Shell脚本：

```bash
#!/bin/bash

# 日志分析工作流脚本
# 用法: ./analyze_logs.sh access.log

if [ $# -ne 1 ]; then
    echo "用法: $0 access_log_file"
    exit 1
fi

LOG_FILE=$1
TEMP_DIR="./temp_$(date +%s)"
REPORT_FILE="report_$(date +%Y%m%d).txt"

# 创建临时目录
mkdir -p $TEMP_DIR

# 步骤1: 提取特定时间段的日志
echo "正在提取时间段内的日志..."
grep "\[15/Jul/2023:10:3[0-5]:" $LOG_FILE > $TEMP_DIR/filtered_by_time.log

# 步骤2: 过滤出错误请求
echo "正在过滤错误请求..."
grep -E "HTTP/1\.[01]\" [45][0-9][0-9]" $TEMP_DIR/filtered_by_time.log > $TEMP_DIR/error_requests.log

# 步骤3: 统计每个IP的访问次数
echo "正在统计IP访问次数..."
awk '{print $1}' $TEMP_DIR/filtered_by_time.log | sort | uniq -c | sort -nr > $TEMP_DIR/ip_counts.txt

# 步骤4: 分析最常访问的URL路径
echo "正在分析URL访问情况..."
awk '{print $7}' $TEMP_DIR/filtered_by_time.log | sort | uniq -c | sort -nr > $TEMP_DIR/url_counts.txt

# 步骤5: 生成综合报告
echo "正在生成报告..."
{
    echo "Web服务器访问日志分析报告"
    echo "=========================="
    echo "生成时间: $(date)"
    echo "分析时间段: 15/Jul/2023 10:30-10:35"
    echo "分析文件: $LOG_FILE"
    echo ""
    
    echo "1. 总体统计"
    echo "------------"
    echo "总请求数: $(wc -l < $TEMP_DIR/filtered_by_time.log)"
    echo "错误请求数: $(wc -l < $TEMP_DIR/error_requests.log)"
    echo "错误率: $(awk "BEGIN {printf \"%.2f%%\", ($(wc -l < $TEMP_DIR/error_requests.log) / $(wc -l < $TEMP_DIR/filtered_by_time.log)) * 100}")"
    echo ""
    
    echo "2. IP访问统计 (前5名)"
    echo "--------------------"
    head -5 $TEMP_DIR/ip_counts.txt
    echo ""
    
    echo "3. URL访问统计 (前5名)"
    echo "--------------------"
    head -5 $TEMP_DIR/url_counts.txt
    echo ""
    
    echo "4. 错误请求详情 (前10条)"
    echo "-----------------------"
    head -10 $TEMP_DIR/error_requests.log
} > $REPORT_FILE

echo "报告已生成: $REPORT_FILE"

# 清理临时文件
rm -rf $TEMP_DIR

echo "分析完成!"
```

## 实战项目二：数据转换工作流

### 项目需求

将CSV格式的销售数据转换为结构化的HTML报表，并进行数据清洗和汇总计算。

### 输入数据

假设我们有以下格式的CSV销售数据：

```
日期,产品ID,产品名称,价格,数量,客户ID
2023-07-01,P001,笔记本电脑,5999.00,2,C1001
2023-07-01,P002,鼠标,99.00,5,C1002
2023-07-02,P003,键盘,299.00,3,C1003
2023-07-02,P001,笔记本电脑,5999.00,1,C1004
2023-07-03,P004,显示器,1499.00,2,C1002
```

### 工作流设计

我们将构建一个数据转换工作流：

1. 数据清洗（处理缺失值、格式错误等）
2. 按日期分组计算销售额
3. 按产品分组计算销售量和销售额
4. 生成HTML格式的销售报表

### 实现步骤

#### 1. 数据清洗

```bash
# 检查并处理缺失值和格式问题
awk -F, '
    NR == 1 {print; next}  # 打印标题行
    {
        # 检查字段数量
        if (NF != 6) {
            print "错误行: " $0 > "/dev/stderr"
            next
        }
        
        # 检查日期格式
        if ($1 !~ /^[0-9]{4}-[0-9]{2}-[0-9]{2}$/) {
            print "日期格式错误: " $0 > "/dev/stderr"
            next
        }
        
        # 检查价格和数量是否为数字
        if ($4 !~ /^[0-9]+(\.[0-9]+)?$/ || $5 !~ /^[0-9]+$/) {
            print "价格或数量格式错误: " $0 > "/dev/stderr"
            next
        }
        
        print
    }
' sales.csv > cleaned_sales.csv
```

#### 2. 按日期分组计算销售额

```bash
# 跳过标题行，按日期分组计算销售额
awk -F, '
    NR > 1 {
        # 计算每行的销售额
        amount = $4 * $5
        
        # 按日期累加销售额
        daily_sales[$1] += amount
        
        # 记录总销售额
        total_sales += amount
    }
    
    END {
        print "日期,销售额"
        for (date in daily_sales) {
            printf "%s,%.2f\n", date, daily_sales[date]
        }
        printf "总计,%.2f\n", total_sales
    }
' cleaned_sales.csv > daily_sales.csv
```

#### 3. 按产品分组计算销售量和销售额

```bash
# 按产品分组计算销售量和销售额
awk -F, '
    NR > 1 {
        # 累加每个产品的销售量和销售额
        product_qty[$3] += $5
        product_amount[$3] += $4 * $5
    }
    
    END {
        print "产品名称,销售量,销售额"
        for (product in product_qty) {
            printf "%s,%d,%.2f\n", product, product_qty[product], product_amount[product]
        }
    }
' cleaned_sales.csv > product_sales.csv
```

#### 4. 生成HTML格式的销售报表

```bash
# 生成HTML报表
{
    # HTML头部
    echo '<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>销售数据报表</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        h1 { color: #333; }
        table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        tr:nth-child(even) { background-color: #f9f9f9; }
    </style>
</head>
<body>
    <h1>销售数据报表</h1>
    <h2>生成时间: '$(date)'</h2>'
    
    # 每日销售额表格
    echo '
    <h2>每日销售额</h2>
    <table>
        <tr>
            <th>日期</th>
            <th>销售额</th>
        </tr>'
    
    # 添加每日销售数据行
    awk -F, '
        NR > 1 {
            print "        <tr>"
            print "            <td>" $1 "</td>"
            print "            <td>￥" $2 "</td>"
            print "        </tr>"
        }
    ' daily_sales.csv
    
    echo '    </table>'
    
    # 产品销售表格
    echo '
    <h2>产品销售统计</h2>
    <table>
        <tr>
            <th>产品名称</th>
            <th>销售量</th>
            <th>销售额</th>
        </tr>'
    
    # 添加产品销售数据行
    awk -F, '
        NR > 1 {
            print "        <tr>"
            print "            <td>" $1 "</td>"
            print "            <td>" $2 "</td>"
            print "            <td>￥" $3 "</td>"
            print "        </tr>"
        }
    ' product_sales.csv
    
    echo '    </table>'
    
    # HTML尾部
    echo '</body>
</html>'
} > sales_report.html
```

### 完整工作流脚本

将上述步骤整合为一个完整的Shell脚本：

```bash
#!/bin/bash

# 销售数据转换工作流脚本
# 用法: ./generate_sales_report.sh sales.csv

if [ $# -ne 1 ]; then
    echo "用法: $0 sales_data.csv"
    exit 1
fi

CSV_FILE=$1
TEMP_DIR="./temp_$(date +%s)"
REPORT_FILE="sales_report_$(date +%Y%m%d).html"

# 创建临时目录
mkdir -p $TEMP_DIR

echo "开始处理销售数据..."

# 步骤1: 数据清洗
echo "正在清洗数据..."
awk -F, '
    NR == 1 {print; next}  # 打印标题行
    {
        # 检查字段数量
        if (NF != 6) {
            print "错误行: " $0 > "/dev/stderr"
            next
        }
        
        # 检查日期格式
        if ($1 !~ /^[0-9]{4}-[0-9]{2}-[0-9]{2}$/) {
            print "日期格式错误: " $0 > "/dev/stderr"
            next
        }
        
        # 检查价格和数量是否为数字
        if ($4 !~ /^[0-9]+(\.[0-9]+)?$/ || $5 !~ /^[0-9]+$/) {
            print "价格或数量格式错误: " $0 > "/dev/stderr"
            next
        }
        
        print
    }
' $CSV_FILE > $TEMP_DIR/cleaned_sales.csv

# 步骤2: 按日期分组计算销售额
echo "正在计算每日销售额..."
awk -F, '
    NR > 1 {
        # 计算每行的销售额
        amount = $4 * $5
        
        # 按日期累加销售额
        daily_sales[$1] += amount
        
        # 记录总销售额
        total_sales += amount
    }
    
    END {
        print "日期,销售额"
        for (date in daily_sales) {
            printf "%s,%.2f\n", date, daily_sales[date]
        }
        printf "总计,%.2f\n", total_sales
    }
' $TEMP_DIR/cleaned_sales.csv > $TEMP_DIR/daily_sales.csv

# 步骤3: 按产品分组计算销售量和销售额
echo "正在计算产品销售统计..."
awk -F, '
    NR > 1 {
        # 累加每个产品的销售量和销售额
        product_qty[$3] += $5
        product_amount[$3] += $4 * $5
    }
    
    END {
        print "产品名称,销售量,销售额"
        for (product in product_qty) {
            printf "%s,%d,%.2f\n", product, product_qty[product], product_amount[product]
        }
    }
' $TEMP_DIR/cleaned_sales.csv > $TEMP_DIR/product_sales.csv

# 步骤4: 生成HTML格式的销售报表
echo "正在生成HTML报表..."
{
    # HTML头部
    echo '<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>销售数据报表</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        h1 { color: #333; }
        table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        tr:nth-child(even) { background-color: #f9f9f9; }
        .total-row { font-weight: bold; background-color: #e6f7ff; }
    </style>
</head>
<body>
    <h1>销售数据报表</h1>
    <h2>生成时间: '$(date)'</h2>'
    
    # 每日销售额表格
    echo '
    <h2>每日销售额</h2>
    <table>
        <tr>
            <th>日期</th>
            <th>销售额</th>
        </tr>'
    
    # 添加每日销售数据行
    awk -F, '
        NR > 1 {
            if ($1 == "总计") {
                print "        <tr class=\"total-row\">"
            } else {
                print "        <tr>"
            }
            print "            <td>" $1 "</td>"
            print "            <td>￥" $2 "</td>"
            print "        </tr>"
        }
    ' $TEMP_DIR/daily_sales.csv
    
    echo '    </table>'
    
    # 产品销售表格
    echo '
    <h2>产品销售统计</h2>
    <table>
        <tr>
            <th>产品名称</th>
            <th>销售量</th>
            <th>销售额</th>
        </tr>'
    
    # 添加产品销售数据行
    awk -F, '
        NR > 1 {
            print "        <tr>"
            print "            <td>" $1 "</td>"
            print "            <td>" $2 "</td>"
            print "            <td>￥" $3 "</td>"
            print "        </tr>"
        }
    ' $TEMP_DIR/product_sales.csv
    
    echo '    </table>'
    
    # HTML尾部
    echo '</body>
</html>'
} > $REPORT_FILE

echo "报表已生成: $REPORT_FILE"

# 清理临时文件
rm -rf $TEMP_DIR

echo "处理完成!"
```

## 实战项目三：系统监控工作流

### 项目需求

构建一个系统监控工作流，定期收集系统资源使用情况，分析性能瓶颈，并生成监控报告。

### 工作流设计

我们将构建一个系统监控工作流：

1. 收集CPU、内存、磁盘和网络使用情况
2. 分析系统日志中的错误和警告
3. 检测异常进程和资源使用
4. 生成系统健康报告

### 实现步骤

#### 1. 收集系统资源使用情况

```bash
#!/bin/bash

# 创建输出目录
OUTPUT_DIR="./system_stats_$(date +%Y%m%d)"
mkdir -p $OUTPUT_DIR

# 收集CPU信息
echo "正在收集CPU信息..."
{
    echo "CPU使用情况 - $(date)"
    echo "--------------------"
    top -bn1 | head -20
    echo ""
    
    echo "CPU负载 - $(date)"
    echo "--------------------"
    uptime
    echo ""
    
    echo "CPU详细信息 - $(date)"
    echo "--------------------"
    cat /proc/cpuinfo | grep -E "processor|model name|cpu MHz|cache size"
} > $OUTPUT_DIR/cpu_info.txt

# 收集内存信息
echo "正在收集内存信息..."
{
    echo "内存使用情况 - $(date)"
    echo "--------------------"
    free -h
    echo ""
    
    echo "内存详细信息 - $(date)"
    echo "--------------------"
    cat /proc/meminfo | head -20
} > $OUTPUT_DIR/memory_info.txt

# 收集磁盘信息
echo "正在收集磁盘信息..."
{
    echo "磁盘使用情况 - $(date)"
    echo "--------------------"
    df -h
    echo ""
    
    echo "磁盘I/O统计 - $(date)"
    echo "--------------------"
    iostat -x 1 5
} > $OUTPUT_DIR/disk_info.txt

# 收集网络信息
echo "正在收集网络信息..."
{
    echo "网络连接情况 - $(date)"
    echo "--------------------"
    netstat -tuln
    echo ""
    
    echo "网络接口统计 - $(date)"
    echo "--------------------"
    ifconfig
    echo ""
    
    echo "网络流量统计 - $(date)"
    echo "--------------------"
    sar -n DEV 1 5
} > $OUTPUT_DIR/network_info.txt

echo "系统资源信息已收集到 $OUTPUT_DIR 目录"
```

#### 2. 分析系统日志中的错误和警告

```bash
#!/bin/bash

# 分析系统日志
LOG_DIR="/var/log"
OUTPUT_DIR="./system_stats_$(date +%Y%m%d)"
mkdir -p $OUTPUT_DIR

echo "正在分析系统日志..."
{
    echo "系统日志错误分析 - $(date)"
    echo "------------------------"
    
    echo "1. 系统错误信息 (最近100条)"
    echo "------------------------"
    grep -i "error\|fail\|critical" $LOG_DIR/syslog | tail -100
    echo ""
    
    echo "2. 认证失败尝试"
    echo "------------------------"
    grep -i "failed\|failure" $LOG_DIR/auth.log | tail -50
    echo ""
    
    echo "3. 内核错误信息"
    echo "------------------------"
    dmesg | grep -i "error\|fail\|warn" | tail -50
    echo ""
    
    echo "4. 应用程序错误 (Apache/Nginx)"
    echo "------------------------"
    if [ -f "$LOG_DIR/apache2/error.log" ]; then
        grep -i "error" $LOG_DIR/apache2/error.log | tail -50
    elif [ -f "$LOG_DIR/nginx/error.log" ]; then
        grep -i "error" $LOG_DIR/nginx/error.log | tail -50
    else
        echo "未找到Web服务器错误日志"
    fi
} > $OUTPUT_DIR/log_analysis.txt

echo "日志分析完成，结果保存在 $OUTPUT_DIR/log_analysis.txt"
```

#### 3. 检测异常进程和资源使用

```bash
#!/bin/bash

OUTPUT_DIR="./system_stats_$(date +%Y%m%d)"
mkdir -p $OUTPUT_DIR

echo "正在检测异常进程和资源使用..."
{
    echo "异常进程和资源使用分析 - $(date)"
    echo "------------------------------"
    
    echo "1. CPU使用率最高的进程 (前10个)"
    echo "------------------------------"
    ps aux | sort -nrk 3,3 | head -10
    echo ""
    
    echo "2. 内存使用率最高的进程 (前10个)"
    echo "------------------------------"
    ps aux | sort -nrk 4,4 | head -10
    echo ""
    
    echo "3. 运行时间最长的进程 (前10个)"
    echo "------------------------------"
    ps -eo pid,user,comm,etime,args | sort -nrk 4,4 | head -10
    echo ""
    
    echo "4. 僵尸进程检测"
    echo "------------------------------"
    ps aux | awk '$8=="Z"' | if [ -z "$(cat)" ]; then echo "没有僵尸进程"; else cat; fi
    echo ""
    
    echo "5. 打开文件数最多的进程 (前5个)"
    echo "------------------------------"
    for pid in $(ps -eo pid --no-headers | head -5); do
        echo "进程 $pid ($(ps -p $pid -o comm=)) 打开的文件数: $(ls -l /proc/$pid/fd 2>/dev/null | wc -l)"
    done
} > $OUTPUT_DIR/process_analysis.txt

echo "进程分析完成，结果保存在 $OUTPUT_DIR/process_analysis.txt"
```

#### 4. 生成系统健康报告

```bash
#!/bin/bash

OUTPUT_DIR="./system_stats_$(date +%Y%m%d)"
REPORT_FILE="$OUTPUT_DIR/system_health_report.html"

echo "正在生成系统健康报告..."

# 提取CPU使用率
CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print $2 + $4}')
CPU_STATUS="正常"
if (( $(echo "$CPU_USAGE > 80" | bc -l) )); then
    CPU_STATUS="警告"
elif (( $(echo "$CPU_USAGE > 90" | bc -l) )); then
    CPU_STATUS="危险"
fi

# 提取内存使用率
MEM_TOTAL=$(free | grep Mem | awk '{print $2}')
MEM_USED=$(free | grep Mem | awk '{print $3}')
MEM_USAGE=$(echo "scale=2; $MEM_USED*100/$MEM_TOTAL" | bc)
MEM_STATUS="正常"
if (( $(echo "$MEM_USAGE > 80" | bc -l) )); then
    MEM_STATUS="警告"
elif (( $(echo "$MEM_USAGE > 90" | bc -l) )); then
    MEM_STATUS="危险"
fi

# 提取磁盘使用率
DISK_USAGE=$(df让我继续完成系统监控工作流的内容：

```bash
# 提取磁盘使用率
DISK_USAGE=$(df -h | grep -vE "tmpfs|cdrom" | awk 'NR>1 {print $5}' | sort -nr | head -1 | sed 's/%//')
DISK_STATUS="正常"
if [ "$DISK_USAGE" -gt 80 ]; then
    DISK_STATUS="警告"
elif [ "$DISK_USAGE" -gt 90 ]; then
    DISK_STATUS="危险"
fi

# 生成HTML报告
{
    echo '<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>系统健康报告</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        h1 { color: #333; }
        .status-normal { color: green; }
        .status-warning { color: orange; }
        .status-danger { color: red; }
        table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        tr:nth-child(even) { background-color: #f9f9f9; }
    </style>
</head>
<body>
    <h1>系统健康报告</h1>
    <h2>生成时间: '$(date)'</h2>
    
    <h2>系统资源概览</h2>
    <table>
        <tr>
            <th>资源</th>
            <th>使用率</th>
            <th>状态</th>
        </tr>
        <tr>
            <td>CPU</td>
            <td>'$CPU_USAGE'%</td>
            <td class="status-'$(echo $CPU_STATUS | tr '[:upper:]' '[:lower:]')'">'$CPU_STATUS'</td>
        </tr>
        <tr>
            <td>内存</td>
            <td>'$MEM_USAGE'%</td>
            <td class="status-'$(echo $MEM_STATUS | tr '[:upper:]' '[:lower:]')'">'$MEM_STATUS'</td>
        </tr>
        <tr>
            <td>磁盘</td>
            <td>'$DISK_USAGE'%</td>
            <td class="status-'$(echo $DISK_STATUS | tr '[:upper:]' '[:lower:]')'">'$DISK_STATUS'</td>
        </tr>
    </table>
    
    <h2>CPU使用率最高的进程</h2>
    <table>
        <tr>
            <th>PID</th>
            <th>用户</th>
            <th>CPU%</th>
            <th>内存%</th>
            <th>命令</th>
        </tr>'
    
    # 添加进程信息
    ps aux | sort -nrk 3,3 | head -5 | awk '{ 
        print "<tr><td>" $2 "</td><td>" $1 "</td><td>" $3 "</td><td>" $4 "</td><td>" $11 "</td></tr>"
    }'
    
    echo '    </table>
    
    <h2>内存使用率最高的进程</h2>
    <table>
        <tr>
            <th>PID</th>
            <th>用户</th>
            <th>CPU%</th>
            <th>内存%</th>
            <th>命令</th>
        </tr>'
    
    # 添加进程信息
    ps aux | sort -nrk 4,4 | head -5 | awk '{ 
        print "<tr><td>" $2 "</td><td>" $1 "</td><td>" $3 "</td><td>" $4 "</td><td>" $11 "</td></tr>"
    }'
    
    echo '    </table>
    
    <h2>系统日志错误摘要</h2>
    <pre>'
    
    # 添加日志错误摘要
    grep -i "error\|fail\|critical" /var/log/syslog 2>/dev/null | tail -10 | sed 's/</\&lt;/g; s/>/\&gt;/g'
    
    echo '    </pre>
</body>
</html>'
} > $REPORT_FILE

echo "系统健康报告已生成: $REPORT_FILE"
```

### 完整工作流脚本

将上述步骤整合为一个完整的Shell脚本：

```bash
#!/bin/bash

# 系统监控工作流脚本
# 用法: ./monitor_system.sh [输出目录]

# 设置输出目录
if [ $# -eq 1 ]; then
    OUTPUT_DIR="$1"
else
    OUTPUT_DIR="./system_stats_$(date +%Y%m%d)"
fi

mkdir -p $OUTPUT_DIR

echo "开始系统监控..."

# 步骤1: 收集系统资源使用情况
echo "正在收集系统资源信息..."

# 收集CPU信息
{
    echo "CPU使用情况 - $(date)"
    echo "--------------------"
    top -bn1 | head -20
    echo ""
    
    echo "CPU负载 - $(date)"
    echo "--------------------"
    uptime
    echo ""
    
    echo "CPU详细信息 - $(date)"
    echo "--------------------"
    cat /proc/cpuinfo | grep -E "processor|model name|cpu MHz|cache size"
} > $OUTPUT_DIR/cpu_info.txt

# 收集内存信息
{
    echo "内存使用情况 - $(date)"
    echo "--------------------"
    free -h
    echo ""
    
    echo "内存详细信息 - $(date)"
    echo "--------------------"
    cat /proc/meminfo | head -20
} > $OUTPUT_DIR/memory_info.txt

# 收集磁盘信息
{
    echo "磁盘使用情况 - $(date)"
    echo "--------------------"
    df -h
    echo ""
    
    echo "磁盘I/O统计 - $(date)"
    echo "--------------------"
    iostat -x 1 5 2>/dev/null || echo "iostat命令不可用"
} > $OUTPUT_DIR/disk_info.txt

# 收集网络信息
{
    echo "网络连接情况 - $(date)"
    echo "--------------------"
    netstat -tuln
    echo ""
    
    echo "网络接口统计 - $(date)"
    echo "--------------------"
    ifconfig 2>/dev/null || ip addr
    echo ""
    
    echo "网络流量统计 - $(date)"
    echo "--------------------"
    sar -n DEV 1 5 2>/dev/null || echo "sar命令不可用"
} > $OUTPUT_DIR/network_info.txt

# 步骤2: 分析系统日志中的错误和警告
echo "正在分析系统日志..."
LOG_DIR="/var/log"
{
    echo "系统日志错误分析 - $(date)"
    echo "------------------------"
    
    echo "1. 系统错误信息 (最近100条)"
    echo "------------------------"
    if [ -f "$LOG_DIR/syslog" ]; then
        grep -i "error\|fail\|critical" $LOG_DIR/syslog | tail -100
    else
        echo "系统日志文件不存在或无法访问"
    fi
    echo ""
    
    echo "2. 认证失败尝试"
    echo "------------------------"
    if [ -f "$LOG_DIR/auth.log" ]; then
        grep -i "failed\|failure" $LOG_DIR/auth.log | tail -50
    else
        echo "认证日志文件不存在或无法访问"
    fi
    echo ""
    
    echo "3. 内核错误信息"
    echo "------------------------"
    dmesg | grep -i "error\|fail\|warn" | tail -50
    echo ""
    
    echo "4. 应用程序错误 (Apache/Nginx)"
    echo "------------------------"
    if [ -f "$LOG_DIR/apache2/error.log" ]; then
        grep -i "error" $LOG_DIR/apache2/error.log | tail -50
    elif [ -f "$LOG_DIR/nginx/error.log" ]; then
        grep -i "error" $LOG_DIR/nginx/error.log | tail -50
    else
        echo "未找到Web服务器错误日志"
    fi
} > $OUTPUT_DIR/log_analysis.txt

# 步骤3: 检测异常进程和资源使用
echo "正在检测异常进程和资源使用..."
{
    echo "异常进程和资源使用分析 - $(date)"
    echo "------------------------------"
    
    echo "1. CPU使用率最高的进程 (前10个)"
    echo "------------------------------"
    ps aux | sort -nrk 3,3 | head -10
    echo ""
    
    echo "2. 内存使用率最高的进程 (前10个)"
    echo "------------------------------"
    ps aux | sort -nrk 4,4 | head -10
    echo ""
    
    echo "3. 运行时间最长的进程 (前10个)"
    echo "------------------------------"
    ps -eo pid,user,comm,etime,args | sort -nrk 4,4 | head -10
    echo ""
    
    echo "4. 僵尸进程检测"
    echo "------------------------------"
    ps aux | awk '$8=="Z"' | if [ -z "$(cat)" ]; then echo "没有僵尸进程"; else cat; fi
    echo ""
    
    echo "5. 打开文件数最多的进程 (前5个)"
    echo "------------------------------"
    for pid in $(ps -eo pid --no-headers | head -5); do
        echo "进程 $pid ($(ps -p $pid -o comm=)) 打开的文件数: $(ls -l /proc/$pid/fd 2>/dev/null | wc -l)"
    done
} > $OUTPUT_DIR/process_analysis.txt

# 步骤4: 生成系统健康报告
echo "正在生成系统健康报告..."
REPORT_FILE="$OUTPUT_DIR/system_health_report.html"

# 提取CPU使用率
CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print $2 + $4}')
CPU_STATUS="正常"
if (( $(echo "$CPU_USAGE > 80" | bc -l) )); then
    CPU_STATUS="警告"
elif (( $(echo "$CPU_USAGE > 90" | bc -l) )); then
    CPU_STATUS="危险"
fi

# 提取内存使用率
MEM_TOTAL=$(free | grep Mem | awk '{print $2}')
MEM_USED=$(free | grep Mem | awk '{print $3}')
MEM_USAGE=$(echo "scale=2; $MEM_USED*100/$MEM_TOTAL" | bc)
MEM_STATUS="正常"
if (( $(echo "$MEM_USAGE > 80" | bc -l) )); then
    MEM_STATUS="警告"
elif (( $(echo "$MEM_USAGE > 90" | bc -l) )); then
    MEM_STATUS="危险"
fi

# 提取磁盘使用率
DISK_USAGE=$(df -h | grep -vE "tmpfs|cdrom" | awk 'NR>1 {print $5}' | sort -nr | head -1 | sed 's/%//')
DISK_STATUS="正常"
if [ "$DISK_USAGE" -gt 80 ]; then
    DISK_STATUS="警告"
elif [ "$DISK_USAGE" -gt 90 ]; then
    DISK_STATUS="危险"
fi

# 生成HTML报告
{
    echo '<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>系统健康报告</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        h1 { color: #333; }
        .status-normal { color: green; }
        .status-warning { color: orange; }
        .status-danger { color: red; }
        table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        tr:nth-child(even) { background-color: #f9f9f9; }
    </style>
</head>
<body>
    <h1>系统健康报告</h1>
    <h2>生成时间: '$(date)'</h2>
    
    <h2>系统资源概览</h2>
    <table>
        <tr>
            <th>资源</th>
            <th>使用率</th>
            <th>状态</th>
        </tr>
        <tr>
            <td>CPU</td>
            <td>'$CPU_USAGE'%</td>
            <td class="status-'$(echo $CPU_STATUS | tr '[:upper:]' '[:lower:]')'">'$CPU_STATUS'</td>
        </tr>
        <tr>
            <td>内存</td>
            <td>'$MEM_USAGE'%</td>
            <td class="status-'$(echo $MEM_STATUS | tr '[:upper:]' '[:lower:]')'">'$MEM_STATUS'</td>
        </tr>
        <tr>
            <td>磁盘</td>
            <td>'$DISK_USAGE'%</td>
            <td class="status-'$(echo $DISK_STATUS | tr '[:upper:]' '[:lower:]')'">'$DISK_STATUS'</td>
        </tr>
    </table>
    
    <h2>CPU使用率最高的进程</h2>
    <table>
        <tr>
            <th>PID</th>
            <th>用户</th>
            <th>CPU%</th>
            <th>内存%</th>
            <th>命令</th>
        </tr>'
    
    # 添加进程信息
    ps aux | sort -nrk 3,3 | head -5 | awk '{ 
        print "<tr><td>" $2 "</td><td>" $1 "</td><td>" $3 "</td><td>" $4 "</td><td>" $11 "</td></tr>"
    }'
    
    echo '    </table>
    
    <h2>内存使用率最高的进程</h2>
    <table>
        <tr>
            <th>PID</th>
            <th>用户</th>
            <th>CPU%</th>
            <th>内存%</th>
            <th>命令</th>
        </tr>'
    
    # 添加进程信息
    ps aux | sort -nrk 4,4 | head -5 | awk '{ 
        print "<tr><td>" $2 "</td><td>" $1 "</td><td>" $3 "</td><td>" $4 "</td><td>" $11 "</td></tr>"
    }'
    
    echo '    </table>
    
    <h2>系统日志错误摘要</h2>
    <pre>'
    
    # 添加日志错误摘要
    if [ -f "/var/log/syslog" ]; then
        grep -i "error\|fail\|critical" /var/log/syslog | tail -10 | sed 's/</\&lt;/g; s/>/\&gt;/g'
    else
        echo "系统日志文件不存在或无法访问"
    fi
    
    echo '    </pre>
</body>
</html>'
} > $REPORT_FILE

echo "系统健康报告已生成: $REPORT_FILE"
echo "所有监控数据已保存到 $OUTPUT_DIR 目录"
echo "监控完成!"
```

## 构建可重用的文本处理模块

为了提高工作流的可重用性，我们可以将常用的文本处理功能封装为独立的Shell函数或脚本模块。这样可以在不同的项目中重复使用这些功能，提高开发效率。

### 设计模块化文本处理库

下面是一个简单的模块化文本处理库设计，包含常用的文本处理函数：

```bash
#!/bin/bash
# textutils.sh - 文本处理工具库

# 检查依赖工具是否存在
check_dependencies() {
    local missing=0
    for cmd in grep sed awk sort uniq cut tr; do
        if ! command -v $cmd &> /dev/null; then
            echo "错误: 缺少必要工具 '$cmd'" >&2
            missing=1
        fi
    done
    
    if [ $missing -eq 1 ]; then
        echo "请安装缺失的工具后再运行" >&2
        return 1
    fi
    
    return 0
}

# 提取匹配特定模式的行
# 用法: extract_lines "pattern" input_file output_file
extract_lines() {
    local pattern="$1"
    local input="$2"
    local output="$3"
    
    if [ ! -f "$input" ]; then
        echo "错误: 输入文件 '$input' 不存在" >&2
        return 1
    fi
    
    grep -E "$pattern" "$input" > "$output"
    return $?
}

# 替换文本
# 用法: replace_text "pattern" "replacement" input_file output_file
replace_text() {
    local pattern="$1"
    local replacement="$2"
    local input="$3"
    local output="$4"
    
    if [ ! -f "$input" ]; then
        echo "错误: 输入文件 '$input' 不存在" >&2
        return 1
    fi
    
    sed "s/$pattern/$replacement/g" "$input" > "$output"
    return $?
}

# 提取特定列
# 用法: extract_columns delimiter column_list input_file output_file
# 例如: extract_columns "," "1,3,5" input.csv output.csv
extract_columns() {
    local delimiter="$1"
    local columns="$2"
    local input="$3"
    local output="$4"
    
    if [ ! -f "$input" ]; then
        echo "错误: 输入文件 '$input' 不存在" >&2
        return 1
    fi
    
    # 将逗号分隔的列表转换为awk格式
    local awk_columns=$(echo "$columns" | sed 's/,/$,/g;s/^/$/;s/,$//')
    
    awk -F"$delimiter" "{print $awk_columns}" "$input" > "$output"
    return $?
}

# 按列排序
# 用法: sort_by_column delimiter column input_file output_file [reverse] [numeric]
# 例如: sort_by_column "," 2 input.csv output.csv 1 1
sort_by_column() {
    local delimiter="$1"
    local column="$2"
    local input="$3"
    local output="$4"
    local reverse="${5:-0}"
    local numeric="${6:-0}"
    
    if [ ! -f "$input" ]; then
        echo "错误: 输入文件 '$input' 不存在" >&2
        return 1
    fi
    
    local sort_opts="-t$delimiter -k$column"
    
    if [ "$numeric" -eq 1 ]; then
        sort_opts="$sort_opts -n"
    fi
    
    if [ "$reverse" -eq 1 ]; then
        sort_opts="$sort_opts -r"
    fi
    
    sort $sort_opts "$input" > "$output"
    return $?
}

# 统计频率
# 用法: count_frequency input_file output_file [column] [delimiter]
count_frequency() {
    local input="$1"
    local output="$2"
    local column="${3:-1}"
    local delimiter="${4:- }"
    
    if [ ! -f "$input" ]; then
        echo "错误: 输入文件 '$input' 不存在" >&2
        return 1
    fi
    
    if [ "$column" -eq 1 ]; then
        cat "$input"
    else
        cut -d"$delimiter" -f"$column" "$input"
    fi | sort | uniq -c | sort -nr > "$output"
    
    return $?
}

# 合并文件
# 用法: join_files delimiter key_column file1 key_column file2 output_file
join_files() {
    local delimiter="$1"
    local key1="$2"
    local file1="$3"
    local key2="$4"
    local file2="$5"
    local output="$6"
    
    if [ ! -f "$file1" ]; then
        echo "错误: 文件 '$file1' 不存在" >&2
        return 1
    fi
    
    if [ ! -f "$file2" ]; then
        echo "错误: 文件 '$file2' 不存在" >&2
        return 1
    fi
    
    join -t"$delimiter" -1 "$key1" -2 "$key2" "$file1" "$file2" > "$output"
    return $?
}

# 数据透视表
# 用法: pivot_table input_file row_column col_column val_column delimiter output_file
pivot_table() {
    local input="$1"
    local row_col="$2"
    local col_col="$3"
    local val_col="$4"
    local delimiter="$5"
    local output="$6"
    
    if [ ! -f "$input" ]; then
        echo "错误: 输入文件 '$input' 不存在" >&2
        return 1
    fi
    
    # 这是一个复杂操作，使用awk实现
    awk -F"$delimiter" '
    BEGIN {
        print "生成数据透视表..."
    }
    {
        # 提取行、列和值
        row = $'$row_col'
        col = $'$col_col'
        val = $'$val_col'
        
        # 记录所有唯一的行和列值
        rows[row] = 1
        cols[col] = 1
        
        # 累加数据
        data[row,col] += val
    }
    END {
        # 打印表头
        printf "Row/Column"
        for (c in cols) {
            printf "'$delimiter'%s", c
        }
        printf "\n"
        
        # 打印数据行
        for (r in rows) {
            printf "%s", r
            for (c in cols) {
                if ((r,c) in data) {
                    printf "'$delimiter'%s", data[r,c]
                } else {
                    printf "'$delimiter'0"
                }
            }
            printf "\n"
        }
    }
    ' "$input" > "$output"
    
    return $?
}

# 检查文件编码并转换为UTF-8
# 用法: convert_to_utf8 input_file output_file
convert_to_utf8() {
    local input="$1"
    local output="$2"
    
    if [ ! -f "$input" ]; then
        echo "错误: 输入文件 '$input' 不存在" >&2
        return 1
    fi
    
    # 检查是否安装了iconv
    if ! command -v iconv &> /dev/null; then
        echo "错误: 缺少必要工具 'iconv'" >&2
        return 1
    fi
    
    # 检查文件编码
    local encoding
    if command -v file &> /dev/null; then
        encoding=$(file -i "$input" | sed 's/.*charset=\([^;]*\).*/\1/')
    else
        # 如果没有file命令，假设是ISO-8859-1
        encoding="ISO-8859-1"
    fi
    
    # 如果已经是UTF-8，直接复制
    if [ "$encoding" = "utf-8" ] || [ "$encoding" = "us-ascii" ]; then
        cp "$input" "$output"
    else
        # 否则转换为UTF-8
        iconv -f "$encoding" -t UTF-8 "$input" > "$output"
    fi
    
    return $?
}
```

### 使用模块化库的示例

下面是一个使用上述模块化库的示例脚本：

```bash
#!/bin/bash

# 引入文本处理库
source ./textutils.sh

# 检查依赖
if ! check_dependencies; then
    exit 1
fi

# 创建临时目录
TEMP_DIR="./temp_$(date +%s)"
mkdir -p $TEMP_DIR

# 处理CSV数据
echo "处理销售数据..."
INPUT_FILE="sales.csv"

# 1. 提取特定列（日期、产品名称、价格、数量）
extract_columns "," "1,3,4,5" "$INPUT_FILE" "$TEMP_DIR/extracted.csv"

# 2. 按日期排序
sort_by_column "," 1 "$TEMP_DIR/extracted.csv" "$TEMP_DIR/sorted.csv"

# 3. 统计产品销售频率
cut -d, -f2 "$TEMP_DIR/sorted.csv" | sort | uniq -c | sort -nr > "$TEMP_DIR/product_frequency.txt"

# 4. 生成报告
{
    echo "销售数据分析报告"
    echo "================="
    echo "生成时间: $(date)"
    echo ""
    
    echo "产品销售频率:"
    cat "$TEMP_DIR/product_frequency.txt"
} > "sales_analysis_report.txt"

# 清理临时文件
rm -rf $TEMP_DIR

echo "处理完成! 报告已保存到 sales_analysis_report.txt"
```

## 工作流自动化与调度

为了实现文本处理工作流的自动化，我们可以使用cron作业定期执行脚本，或者创建触发器在特定事件发生时执行处理流程。

### 使用cron定期执行工作流

```bash
# 编辑crontab
crontab -e

# 添加以下内容，每天凌晨2点执行日志分析
0 2 * * * /path/to/analyze_logs.sh /var/log/apache2/access.log > /dev/null 2>&1

# 每周一早上8点生成销售报表
0 8 * * 1 /path/to/generate_sales_report.sh /data/weekly_sales.csv > /dev/null 2>&1

# 每小时执行一次系统监控
0 * * * * /path/to/monitor_system.sh /var/log/system_stats > /dev/null 2>&1
```

### 创建文件系统触发器

使用inotify-tools可以监控文件系统变化并触发处理流程：

```bash
#!/bin/bash

# 监控目录
WATCH_DIR="/data/incoming"
# 处理脚本
PROCESS_SCRIPT="/path/to/process_data.sh"

# 安装inotify-tools
# apt-get install inotify-tools

# 监控目录中的新文件
inotifywait -m -e close_write --format '%w%f' "$WATCH_DIR" | while read file
do
    echo "检测到新文件: $file"
    "$PROCESS_SCRIPT" "$file"
done
```

## 工作流性能优化

处理大型文本文件时，性能是一个重要考虑因素。以下是一些优化技巧：

### 减少磁盘I/O

```bash
# 使用内存文件系统处理临时文件
TEMP_DIR="/dev/shm/temp_$(date +%s)"
mkdir -p $TEMP_DIR

# 处理完成后清理
trap "rm -rf $TEMP_DIR" EXIT
```

### 并行处理

使用GNU Parallel可以并行执行任务，充分利用多核CPU：

```bash
#!/bin/bash

# 安装parallel
# apt-get install parallel

# 要处理的文件列表
find /data/logs -name "*.log" > log_files.txt

# 并行处理每个文件
cat log_files.txt | parallel -j 4 './process_log.sh {}'
```

### 使用合适的工具

对于非常大的文件，可以使用专门的工具如`split`、`csplit`将文件分割成小块处理：

```bash
# 将大文件分割成每个100MB的小文件
split -b 100M large_file.csv split_

# 处理每个分割文件
for file in split_*; do
    ./process_chunk.sh "$file"
done

# 合并结果
cat result_split_* > final_result.txt
```

## 工作流错误处理与日志

健壮的工作流应该包含完善的错误处理和日志记录机制：

```bash
#!/bin/bash

# 日志函数
log() {
    local level="$1"
    local message="$2"
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] [$level] $message" | tee -a workflow.log
}

# 错误处理
handle_error() {
    local exit_code=$?
    log "ERROR" "脚本执行失败，退出码: $exit_code"
    # 清理临时文件
    [ -d "$TEMP_DIR" ] && rm -rf "$TEMP_DIR"
    exit $exit_code
}

# 设置错误处理陷阱让我继续完成工作流错误处理与日志部分的内容：

```bash
# 设置错误处理陷阱
trap handle_error ERR

# 创建临时目录
TEMP_DIR="./temp_$(date +%s)"
mkdir -p $TEMP_DIR || {
    log "ERROR" "无法创建临时目录"
    exit 1
}

# 执行工作流步骤
log "INFO" "开始处理数据..."

# 步骤1: 检查输入文件
if [ ! -f "$INPUT_FILE" ]; then
    log "ERROR" "输入文件不存在: $INPUT_FILE"
    exit 1
fi

# 步骤2: 处理数据
log "INFO" "正在处理数据..."
if ! process_data "$INPUT_FILE" "$TEMP_DIR/processed.txt"; then
    log "ERROR" "数据处理失败"
    exit 1
fi

# 步骤3: 生成报告
log "INFO" "正在生成报告..."
if ! generate_report "$TEMP_DIR/processed.txt" "$OUTPUT_FILE"; then
    log "ERROR" "报告生成失败"
    exit 1
fi

# 清理临时文件
rm -rf "$TEMP_DIR"

log "INFO" "处理完成，输出文件: $OUTPUT_FILE"
```

## 工作流文档化

良好的文档对于工作流的维护和共享至关重要。以下是一个工作流文档模板：

```markdown
# 文本处理工作流文档

## 概述
本工作流用于处理[描述数据类型]数据，实现[描述主要功能]。

## 依赖项
- bash (4.0+)
- grep, sed, awk
- [其他依赖工具]

## 输入
- 文件格式: [描述输入文件格式]
- 示例: [提供输入示例]

## 输出
- 文件格式: [描述输出文件格式]
- 示例: [提供输出示例]

## 使用方法
```bash
./workflow.sh input_file output_file [options]
```

### 参数说明
- `input_file`: 输入文件路径
- `output_file`: 输出文件路径
- `options`: 可选参数
  - `-v`: 详细模式
  - `-d`: 调试模式

## 处理流程
1. [步骤1描述]
2. [步骤2描述]
3. [步骤3描述]

## 错误处理
- 错误代码1: [描述错误情况和解决方法]
- 错误代码2: [描述错误情况和解决方法]

## 示例
```bash
# 基本用法
./workflow.sh data.csv report.html

# 详细模式
./workflow.sh -v data.csv report.html
```

## 维护
- 作者: [作者名]
- 最后更新: [日期]
- 版本: [版本号]
```

## 实战项目四：多源数据整合工作流

### 项目需求

整合多个不同格式的数据源（CSV、JSON、日志文件），提取关键信息，合并数据，并生成统一的分析报告。

### 输入数据

假设我们有以下数据源：

1. 用户数据（CSV格式）：
```
用户ID,姓名,邮箱,注册日期
U001,张三,zhangsan@example.com,2023-01-15
U002,李四,lisi@example.com,2023-02-20
U003,王五,wangwu@example.com,2023-03-05
```

2. 订单数据（JSON格式）：
```json
[
  {
    "order_id": "O001",
    "user_id": "U001",
    "products": [
      {"product_id": "P001", "quantity": 2, "price": 99.00},
      {"product_id": "P002", "quantity": 1, "price": 199.00}
    ],
    "total": 397.00,
    "date": "2023-04-10"
  },
  {
    "order_id": "O002",
    "user_id": "U002",
    "products": [
      {"product_id": "P003", "quantity": 1, "price": 299.00}
    ],
    "total": 299.00,
    "date": "2023-04-15"
  }
]
```

3. 产品访问日志（日志格式）：
```
[2023-04-10 10:15:30] U001 访问了产品 P001
[2023-04-10 10:20:45] U001 访问了产品 P002
[2023-04-10 11:30:22] U001 将产品 P001 加入购物车
[2023-04-15 09:45:10] U002 访问了产品 P003
[2023-04-15 10:05:33] U002 将产品 P003 加入购物车
```

### 工作流设计

我们将构建一个多源数据整合工作流：

1. 解析和清洗各种格式的数据
2. 提取关键信息并转换为统一格式
3. 合并数据，建立用户行为和购买路径
4. 生成用户行为分析报告

### 实现步骤

#### 1. 解析CSV用户数据

```bash
#!/bin/bash

# 解析CSV用户数据
parse_users() {
    local input="$1"
    local output="$2"
    
    # 跳过标题行，提取用户信息
    awk -F, 'NR > 1 {
        # 输出JSON格式的用户数据
        printf "{\n"
        printf "  \"user_id\": \"%s\",\n", $1
        printf "  \"name\": \"%s\",\n", $2
        printf "  \"email\": \"%s\",\n", $3
        printf "  \"register_date\": \"%s\"\n", $4
        printf "}\n"
    }' "$input" | jq -s '.' > "$output"
    
    echo "用户数据解析完成: $(jq '. | length' "$output") 条记录"
}
```

#### 2. 解析JSON订单数据

```bash
#!/bin/bash

# 解析JSON订单数据
parse_orders() {
    local input="$1"
    local output="$2"
    
    # 使用jq提取订单信息
    jq '[.[] | {
        order_id: .order_id,
        user_id: .user_id,
        product_count: (.products | length),
        products: [.products[] | .product_id],
        total: .total,
        date: .date
    }]' "$input" > "$output"
    
    echo "订单数据解析完成: $(jq '. | length' "$output") 条记录"
}
```

#### 3. 解析日志文件

```bash
#!/bin/bash

# 解析访问日志
parse_logs() {
    local input="$1"
    local output="$2"
    
    # 提取日志中的用户行为
    grep -E '\[[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}\] [A-Z][0-9]{3} ' "$input" | 
    sed -E 's/\[([0-9]{4}-[0-9]{2}-[0-9]{2}) ([0-9]{2}:[0-9]{2}:[0-9]{2})\] ([A-Z][0-9]{3}) (.+)/\1|\2|\3|\4/' |
    awk -F'|' '{
        # 提取日期、时间、用户ID和行为
        date = $1
        time = $2
        user_id = $3
        action = $4
        
        # 提取产品ID
        if (action ~ /产品 ([A-Z][0-9]{3})/) {
            match(action, /产品 ([A-Z][0-9]{3})/, arr)
            product_id = arr[1]
        } else {
            product_id = ""
        }
        
        # 确定行为类型
        if (action ~ /访问/) {
            action_type = "view"
        } else if (action ~ /加入购物车/) {
            action_type = "add_to_cart"
        } else {
            action_type = "other"
        }
        
        # 输出JSON格式
        printf "{\n"
        printf "  \"date\": \"%s\",\n", date
        printf "  \"time\": \"%s\",\n", time
        printf "  \"user_id\": \"%s\",\n", user_id
        printf "  \"product_id\": \"%s\",\n", product_id
        printf "  \"action\": \"%s\"\n", action_type
        printf "}\n"
    }' | jq -s '.' > "$output"
    
    echo "日志数据解析完成: $(jq '. | length' "$output") 条记录"
}
```

#### 4. 合并数据并生成用户行为路径

```bash
#!/bin/bash

# 合并数据并生成用户行为路径
generate_user_paths() {
    local users="$1"
    local orders="$2"
    local logs="$3"
    local output="$4"
    
    # 使用jq合并数据
    jq -s '
    # 首先加载所有数据
    {
        users: .[0],
        orders: .[1],
        logs: .[2]
    } |
    
    # 为每个用户生成行为路径
    {
        user_paths: [
            .users[] | {
                user_id: .user_id,
                name: .name,
                register_date: .register_date,
                
                # 查找该用户的所有订单
                orders: [.orders[] | select(.user_id == .user_id)],
                
                # 查找该用户的所有行为日志，并按时间排序
                activities: [.logs[] | select(.user_id == .user_id)] | sort_by(.date, .time),
                
                # 计算统计信息
                stats: {
                    view_count: [.logs[] | select(.user_id == .user_id and .action == "view")] | length,
                    cart_count: [.logs[] | select(.user_id == .user_id and .action == "add_to_cart")] | length,
                    order_count: [.orders[] | select(.user_id == .user_id)] | length,
                    total_spent: ([.orders[] | select(.user_id == .user_id) | .total] | add // 0)
                }
            }
        ]
    }
    ' "$users" "$orders" "$logs" > "$output"
    
    echo "用户行为路径生成完成: $(jq '.user_paths | length' "$output") 个用户"
}
```

#### 5. 生成HTML分析报告

```bash
#!/bin/bash

# 生成HTML分析报告
generate_report() {
    local data="$1"
    local output="$2"
    
    # 创建HTML报告
    {
        echo '<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>用户行为分析报告</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        h1, h2, h3 { color: #333; }
        table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        tr:nth-child(even) { background-color: #f9f9f9; }
        .user-section { margin-bottom: 30px; border: 1px solid #eee; padding: 15px; border-radius: 5px; }
        .stats { display: flex; justify-content: space-between; }
        .stat-box { background-color: #f5f5f5; padding: 10px; border-radius: 5px; width: 22%; text-align: center; }
        .timeline { margin-top: 20px; }
        .timeline-item { margin-bottom: 10px; padding-left: 20px; border-left: 2px solid #ddd; }
        .view { color: blue; }
        .add_to_cart { color: green; }
        .purchase { color: red; }
    </style>
</head>
<body>
    <h1>用户行为分析报告</h1>
    <h2>生成时间: '$(date)'</h2>'
        
        # 添加用户数据
        jq -r '.user_paths[] | "
    <div class=\"user-section\">
        <h2>用户: \(.name) (\(.user_id))</h2>
        <p>注册日期: \(.register_date)</p>
        
        <div class=\"stats\">
            <div class=\"stat-box\">
                <h3>浏览次数</h3>
                <p>\(.stats.view_count)</p>
            </div>
            <div class=\"stat-box\">
                <h3>加购次数</h3>
                <p>\(.stats.cart_count)</p>
            </div>
            <div class=\"stat-box\">
                <h3>订单数</h3>
                <p>\(.stats.order_count)</p>
            </div>
            <div class=\"stat-box\">
                <h3>总消费</h3>
                <p>￥\(.stats.total_spent)</p>
            </div>
        </div>
        
        <h3>订单历史</h3>
        <table>
            <tr>
                <th>订单ID</th>
                <th>日期</th>
                <th>产品数量</th>
                <th>总金额</th>
            </tr>
            \(.orders[] | "
            <tr>
                <td>\(.order_id)</td>
                <td>\(.date)</td>
                <td>\(.product_count)</td>
                <td>￥\(.total)</td>
            </tr>
            ")
        </table>
        
        <h3>用户行为时间线</h3>
        <div class=\"timeline\">
            \(.activities[] | "
            <div class=\"timeline-item \(.action)\">
                <p><strong>\(.date) \(.time)</strong>: \(if .action == \"view\" then \"浏览产品\" else \"加入购物车\" end) \(.product_id)</p>
            </div>
            ")
            \(.orders[] | "
            <div class=\"timeline-item purchase\">
                <p><strong>\(.date)</strong>: 购买了 \(.product_count) 件商品，总金额 ￥\(.total)</p>
            </div>
            ")
        </div>
    </div>
        "' "$data"
        
        # HTML尾部
        echo '</body>
</html>'
    } > "$output"
    
    echo "分析报告已生成: $output"
}
```

### 完整工作流脚本

将上述步骤整合为一个完整的Shell脚本：

```bash
#!/bin/bash

# 多源数据整合工作流脚本
# 用法: ./integrate_data.sh

# 设置工作目录
WORK_DIR="./data_integration_$(date +%s)"
mkdir -p $WORK_DIR

# 输入文件
USERS_CSV="./users.csv"
ORDERS_JSON="./orders.json"
LOGS_FILE="./access.log"

# 输出文件
REPORT_FILE="user_behavior_report_$(date +%Y%m%d).html"

# 日志函数
log() {
    local level="$1"
    local message="$2"
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] [$level] $message"
}

# 错误处理
handle_error() {
    local exit_code=$?
    log "ERROR" "脚本执行失败，退出码: $exit_code"
    # 清理临时文件
    [ -d "$WORK_DIR" ] && rm -rf "$WORK_DIR"
    exit $exit_code
}

# 设置错误处理陷阱
trap handle_error ERR

# 检查依赖
for cmd in jq awk grep sed; do
    if ! command -v $cmd &> /dev/null; then
        log "ERROR" "缺少必要工具: $cmd"
        exit 1
    fi
done

# 检查输入文件
for file in "$USERS_CSV" "$ORDERS_JSON" "$LOGS_FILE"; do
    if [ ! -f "$file" ]; then
        log "ERROR" "输入文件不存在: $file"
        exit 1
    fi
done

log "INFO" "开始数据整合工作流..."

# 步骤1: 解析CSV用户数据
log "INFO" "正在解析用户数据..."
parse_users "$USERS_CSV" "$WORK_DIR/users.json"

# 步骤2: 解析JSON订单数据
log "INFO" "正在解析订单数据..."
parse_orders "$ORDERS_JSON" "$WORK_DIR/orders.json"

# 步骤3: 解析日志文件
log "INFO" "正在解析访问日志..."
parse_logs "$LOGS_FILE" "$WORK_DIR/logs.json"

# 步骤4: 合并数据并生成用户行为路径
log "INFO" "正在生成用户行为路径..."
generate_user_paths "$WORK_DIR/users.json" "$WORK_DIR/orders.json" "$WORK_DIR/logs.json" "$WORK_DIR/user_paths.json"

# 步骤5: 生成HTML分析报告
log "INFO" "正在生成分析报告..."
generate_report "$WORK_DIR/user_paths.json" "$REPORT_FILE"

# 清理临时文件
rm -rf "$WORK_DIR"

log "INFO" "数据整合工作流完成，报告已生成: $REPORT_FILE"
```

## 总结

通过本文的实战项目，我们学习了如何构建高效的文本处理工作流，整合多种工具和技术，实现复杂的文本分析和处理任务。关键要点包括：

1. **工具链组合**：合理组合grep、sed、awk等工具，发挥各自优势
2. **模块化设计**：将复杂任务分解为独立模块，提高可维护性和可重用性
3. **管道化处理**：充分利用Unix管道机制，实现数据流的高效处理
4. **错误处理**：加入完善的错误检测和处理机制，提高工作流的健壮性
5. **性能优化**：针对大文件处理进行优化设计，提高处理效率
6. **自动化调度**：使用cron或文件系统触发器实现工作流的自动化执行

通过掌握这些技能，你可以构建出强大而灵活的文本处理工作流，大幅提高数据处理效率和自动化程度，为日常工作和项目开发提供有力支持。